{
  "metadata": {
    "name": "dashboard",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 파일 불러오기"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark.stop()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 스파크 세션 생성\nfrom pyspark.sql import SparkSession\n\n# 인스턴스 생성(Max Memory 지정: Out of Memory 방지)\nMAX_MEMORY\u003d\"4g\"\nspark \u003d SparkSession.builder.appName(\"taxi-fare\")\\\n                .config(\"spark.executor.memory\", MAX_MEMORY)\\\n                .config(\"spark.driver.memory\", MAX_MEMORY)\\\n                .config(\"spark.memory.useLegacyMode\", \"true\")\\\n                .config(\"spark.executor.instances\", 2)\\\n                .getOrCreate()\n                \nsc \u003d spark.sparkContext  \n\n# 메모리 상태 확인\nstorage_info \u003d sc.getConf().getAll()\n\nfor item in storage_info:\n    print(item)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 일일 시간대별 택시 선호도 "
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nspark.catalog.clearCache()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n#yellow_df \u003d spark.read.parquet(\"/home/kim/data/NY_taxi/tripdata/yellow_tripdata/2021/yellow_tripdata_2021-01.parquet\", inferSchema\u003dTrue, header\u003dTrue) # 여러개 파일 동시에 가져올 수 있다.\nyellow_df \u003d spark.read.parquet(\"/home/kim/data/NY_taxi/tripdata/yellow_tripdata/2021\", inferSchema\u003dTrue, header\u003dTrue) # 여러개 파일 동시에 가져올 수 있다.\n\nzone_df \u003d spark.read.csv(\"/home/kim/data/NY_taxi/taxi_zone_lookup.csv\", inferSchema\u003dTrue, header\u003dTrue)\n\nweather_df \u003d spark.read.csv(\"/home/kim/data/NYC_Weather_2016_2022.csv\", inferSchema\u003dTrue, header\u003dTrue)\n\n\nyellow_df.createOrReplaceTempView(\"yellow\")\nzone_df.createOrReplaceTempView(\"zone\")\n\nweather_df.createOrReplaceTempView(\"weather\")\nweather_df.select(\"cloudcover (%)\").where(weather_df[\u0027time\u0027] \u003d\u003d \"2021-01-02 06:00:00\").show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nyellow_df.cache()\nweather_df.cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \"\"\"\nSELECT \n    y.VendorID as vendor_id,\n    TO_DATE(y.tpep_pickup_datetime) as pickup_date,\n    TO_DATE(y.tpep_dropoff_datetime) as dropoff_date,\n    HOUR(y.tpep_pickup_datetime) as pickup_time,\n    HOUR(y.tpep_dropoff_datetime) as dropoff_time,\n    y.passenger_count,\n    y.trip_distance,\n    y.fare_amount,\n    y.tip_amount,\n    y.tolls_amount,\n    y.total_amount,\n    y.payment_type,\n    pz.Zone as pickup_zone,\n    dz.Zone as dropoff_zone\nFROM \n    yellow y\n    LEFT JOIN \n        zone pz\n    ON\n        y.PULocationID \u003d pz.LocationID\n    LEFT JOIN\n        zone dz\n    ON \n        y.DOLocationID \u003d dz.LocationID\n\"\"\"\ntaxicomb_df \u003d spark.sql(query)\ntaxicomb_df.unpersist()\n\n# 새로운 TempView에 담는다.\ntaxicomb_df.createOrReplaceTempView(\"taxicomb\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntaxicomb_df.cache()\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### 날씨 데이터 전처리하기 \n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \"\"\"\nSELECT\n    TO_DATE(w.time) as date,\n    HOUR(w.time) as hour,\n    w.`temperature_2m (°C)` as temperature,\n    w.`rain (mm)` as rain,\n    w.`cloudcover (%)` as cloudcover\nFROM \n    weather w\nWHERE TO_DATE(w.time) \u003e \u00272021-01-01\u0027 AND TO_DATE(w.time) \u003c \u00272021-02-01\u0027\n\"\"\"\nweather_df\u003d spark.sql(query)\nweather_df.createOrReplaceTempView(\"weather\")\n\n\nweather_df.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### weather랑 join하기 "
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nimport time\nfrom pyspark import StorageLevel\nfrom pyspark.sql import functions as F\n\nquery \u003d \"\"\"\nSELECT\n    *\nFROM \n    taxicomb tc\nINNER JOIN\n    weather w\nON\n    tc.pickup_time \u003d w.hour AND tc.pickup_date \u003d w.date\n    \n\"\"\"\ncomb_df\u003d spark.sql(query)\n\n\n#comb_df.persist(StorageLevel.DISK_ONLY)\ncomb_df.cache()\n#comb_df.unpersist()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nstart \u003d time.time()\n\ncomb_df.show()\n\nend \u003d time.time()\nㄴ\nelapsed_time \u003d end - start\nprint(f\"경과 시간: {elapsed_time:.2f}초\")\n\n# 캐싱 여부 확인\nprint(f\"comb_df is cached? : {comb_df.is_cached}\")  # 캐싱 여부 및 저장 수준 확인\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nstart \u003d time.time()\n\n\n# 시간대별, 픽업 존별로 데이터를 그룹화\n#grouped_df \u003d comb_df.groupBy(\"pickup_time\").agg(F.sum(\"pickup_count\").alias(\"total_count\"))\ngrouped_df \u003d comb_df.groupBy(\"pickup_time\").agg(\n    F.count(\"*\").alias(\"trip_count\"),        # 운행 횟수\n    F.sum(\"total_amount\").alias(\"total_fare\"),  # 총 수입\n    F.avg(\"tip_amount\").alias(\"avg_tip\"),     # 평균 팁\n    F.sum(\"trip_distance\").alias(\"total_distance\")  # 총 운행 거리\n)\n\nend \u003d time.time()\n\nelapsed_time \u003d end - start\nprint(f\"경과 시간: {elapsed_time:.2f}초\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nplt.rc(\u0027font\u0027, family\u003d\u0027NanumGothic\u0027)\nmatplotlib.rcParams[\u0027axes.unicode_minus\u0027] \u003d False\n\n# Pandas로 변환\npandas_df \u003d grouped_df.toPandas()\n\n# pickup_time을 기준으로 정렬\npandas_df \u003d pandas_df.sort_values(by\u003d\"pickup_time\")"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# 꺾은선 그래프 그리기\nplt.figure(figsize\u003d(10, 6))\n\n# 총 운행 거리\nplt.plot(pandas_df[\"pickup_time\"], pandas_df[\"total_distance\"], marker\u003d\"o\", label\u003d\"총 거리\")\n\n# 총 운임\nplt.plot(pandas_df[\"pickup_time\"], pandas_df[\"total_fare\"], marker\u003d\"o\", label\u003d\"총 요금량\")\n\n# 운행 횟수\nplt.plot(pandas_df[\"pickup_time\"], pandas_df[\"trip_count\"], marker\u003d\"o\", label\u003d\"이용 수\")\n\n# 평균 팁\nplt.plot(pandas_df[\"pickup_time\"], pandas_df[\"avg_tip\"], marker\u003d\"o\", label\u003d\"평균 팁\")\n\n# 그래프 타이틀 및 레이블\nplt.title(\"일일 택시 사용량\")\nplt.xlabel(\"픽업 시간\")\nplt.ylabel(\"사용량\")\nplt.legend()\n\n# 그래프 보여주기\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nquery \u003d \"\"\"\nSELECT\n    *\nFROM \n    taxicomb tc\nINNER JOIN\n    weather w\nON\n    tc.pickup_time \u003d w.hour AND tc.pickup_date \u003d w.date;\n\"\"\"\n\ncomb_df \u003d spark.sql(query)\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "특정 시간대의 상위 3개\n"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# PySpark 환경에서 데이터프레임을 불러온다고 가정\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\n# Spark session 생성\nspark \u003d SparkSession.builder.appName(\"stacked_bar_chart\").getOrCreate()\n\nquery \u003d \"\"\"\nWITH RankedPickups AS (\n  SELECT\n    pickup_time,\n    pickup_zone,\n    COUNT(*) AS pickup_count,\n    ROW_NUMBER() OVER (PARTITION BY pickup_time ORDER BY COUNT(*) DESC) AS rank\n  FROM taxicomb\n  WHERE pickup_date \u003d \u00272021-01-02\u0027\n  GROUP BY pickup_time, pickup_zone\n)\nSELECT\n  pickup_time,\n  pickup_zone,\n  pickup_count\nFROM RankedPickups\nWHERE rank \u003c\u003d 1\nORDER BY pickup_time, pickup_count DESC;\n\"\"\"\n# 데이터프레임 불러오기 (여기서는 가정된 예시)\ndf \u003d spark.sql(query)\n\n# 시간대별, 픽업 존별로 데이터를 그룹화\ngrouped_df \u003d df.groupBy(\"pickup_time\", \"pickup_zone\").agg(F.sum(\"pickup_count\").alias(\"total_count\"))\n\n# Pandas로 변환\npandas_df \u003d grouped_df.toPandas()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nplt.rc(\u0027font\u0027, family\u003d\u0027NanumGothic\u0027)\nmatplotlib.rcParams[\u0027axes.unicode_minus\u0027] \u003d False\n\n# 피벗 테이블로 변환하여 시간대별 구역 데이터를 준비\npivot_df \u003d pandas_df.pivot(index\u003d\u0027pickup_time\u0027, columns\u003d\u0027pickup_zone\u0027, values\u003d\u0027total_count\u0027)\n\n# NaN 값을 0으로 변경 (픽업이 없을 경우)\npivot_df \u003d pivot_df.fillna(0)\n\n# 차트 그리기\npivot_df.plot(kind\u003d\u0027bar\u0027, stacked\u003dTrue, figsize\u003d(10, 6))\n\n# 차트 타이틀 및 레이블\nplt.title(\u0027일일 택시 이용객의 선호 출발지\u0027)\nplt.xlabel(\u0027이용 시간\u0027)\nplt.ylabel(\u0027이용객 수\u0027)\nplt.legend(title\u003d\u0027Pickup Zone\u0027)\n\n# 차트 보여주기\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\n# PySpark 환경에서 데이터프레임을 불러온다고 가정\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\n\n# Spark session 생성\nspark \u003d SparkSession.builder.appName(\"stacked_bar_chart\").getOrCreate()\n\nquery \u003d \"\"\"\nWITH RankedPickups AS (\n  SELECT\n    dropoff_time,\n    dropoff_zone,\n    COUNT(*) AS dropoff_count,\n    ROW_NUMBER() OVER (PARTITION BY dropoff_time ORDER BY COUNT(*) DESC) AS rank\n  FROM taxicomb\n  WHERE dropoff_date \u003d \u00272021-01-02\u0027\n  GROUP BY dropoff_time, dropoff_zone\n)\nSELECT\n  dropoff_time,\n  dropoff_zone,\n  dropoff_count\nFROM RankedPickups\nWHERE rank \u003c\u003d 1\nORDER BY dropoff_time, dropoff_count DESC;\n\"\"\"\n# 데이터프레임 불러오기 (여기서는 가정된 예시)\ndf \u003d spark.sql(query)\n\n# 시간대별, 픽업 존별로 데이터를 그룹화\ngrouped_df \u003d df.groupBy(\"dropoff_time\", \"dropoff_zone\").agg(F.sum(\"dropoff_count\").alias(\"total_count\"))\n\n# Pandas로 변환\npandas_df \u003d grouped_df.toPandas()"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nplt.rc(\u0027font\u0027, family\u003d\u0027NanumGothic\u0027)\nmatplotlib.rcParams[\u0027axes.unicode_minus\u0027] \u003d False\n\n# 피벗 테이블로 변환하여 시간대별 구역 데이터를 준비\npivot_df \u003d pandas_df.pivot(index\u003d\u0027dropoff_time\u0027, columns\u003d\u0027dropoff_zone\u0027, values\u003d\u0027total_count\u0027)\n\n# NaN 값을 0으로 변경 (픽업이 없을 경우)\npivot_df \u003d pivot_df.fillna(0)\n\n# 차트 그리기\npivot_df.plot(kind\u003d\u0027bar\u0027, stacked\u003dTrue, figsize\u003d(10, 6))\n\n# 차트 타이틀 및 레이블\nplt.title(\u0027일일 택시 이용객의 선호 도착지\u0027)\nplt.xlabel(\u0027도착 시간\u0027)\nplt.ylabel(\u0027도착 횟수\u0027)\nplt.legend(title\u003d\u0027Dropoff Zone\u0027)\n\n# 차트 보여주기\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}